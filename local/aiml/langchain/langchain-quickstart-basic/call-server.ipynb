{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82ea0b9-5467-43b4-b876-bf173e3dc894",
   "metadata": {},
   "source": [
    "# Interact with a LangServe API\n",
    "\n",
    "Let's try interacting with a LangServe API (defined in `serve.py` running at `localhost:8000`.\n",
    "\n",
    "**Note**: To run `serve.py` successfully, make sure you have exported API keys for OpenAI and Tavily, like this:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"key\"\n",
    "export TAVILY_API_KEY=\"key\"\n",
    "python serve.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43331170-5452-47d1-9caa-0afa436722cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet pip\n",
    "%pip install --upgrade --quiet langserve fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4dafca-4597-4011-b51b-95c9d96d367e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': \"LangSmith can help with testing in various ways throughout the application development lifecycle. Here are some key ways LangSmith can assist with testing:\\n\\n1. Prototyping: LangSmith supports quick experimentation between prompts, model types, retrieval strategies, and other parameters for LLM applications.\\n\\n2. Debugging: LangSmith provides tracing capabilities that allow developers to debug issues by looking through application traces and identifying root causes of problems.\\n\\n3. Initial Test Set: Developers can create datasets of inputs and reference outputs to run tests on LLM applications, helping to ensure the application functions as expected.\\n\\n4. Comparison View: LangSmith offers a comparison view for test runs, allowing developers to track and diagnose regressions in test scores across multiple revisions of the application.\\n\\n5. Playground: LangSmith provides a playground environment for rapid iteration and experimentation with different prompts and models.\\n\\n6. Beta Testing: Developers can collect data on how their LLM applications perform in real-world scenarios during beta testing, helping to track regressions and improvements.\\n\\n7. Capturing Feedback: LangSmith allows developers to gather human feedback on the responses produced by the application and attach feedback scores to logged traces.\\n\\n8. Annotating Traces: LangSmith supports sending runs to annotation queues for annotators to inspect and annotate interesting traces with different criteria.\\n\\n9. Adding Runs to a Dataset: Developers can add runs as examples to datasets to expand test coverage on real-world scenarios and refine the application's performance.\\n\\n10. Production Monitoring: LangSmith enables developers to closely inspect key data points, benchmarking datasets, and monitor application performance with respect to latency, cost, and feedback scores in production.\\n\\n11. Monitoring and A/B Testing: LangSmith provides monitoring charts to track key metrics over time and allows for A/B testing changes in prompt, model, or retrieval strategy.\\n\\n12. Automations: Automations in LangSmith allow developers to perform actions on traces in near real-time, such as scoring traces, sending them to annotation queues, or datasets.\\n\\nOverall, LangSmith offers a comprehensive set of tools and features to support testing and monitoring of LLM applications throughout the development process.\"}\n"
     ]
    }
   ],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")\n",
    "out = remote_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"chat_history\": []  # Providing an empty list as this is the first call\n",
    "})\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b669af7-fc10-4aef-9915-d69ca05a988f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing in various ways throughout the application development lifecycle. Here are some key ways LangSmith can assist with testing:\n",
      "\n",
      "1. Prototyping: LangSmith supports quick experimentation between prompts, model types, retrieval strategies, and other parameters for LLM applications.\n",
      "\n",
      "2. Debugging: LangSmith provides tracing capabilities that allow developers to debug issues by looking through application traces and identifying root causes of problems.\n",
      "\n",
      "3. Initial Test Set: Developers can create datasets of inputs and reference outputs to run tests on LLM applications, helping to ensure the application functions as expected.\n",
      "\n",
      "4. Comparison View: LangSmith offers a comparison view for test runs, allowing developers to track and diagnose regressions in test scores across multiple revisions of the application.\n",
      "\n",
      "5. Playground: LangSmith provides a playground environment for rapid iteration and experimentation with different prompts and models.\n",
      "\n",
      "6. Beta Testing: Developers can collect data on how their LLM applications perform in real-world scenarios during beta testing, helping to track regressions and improvements.\n",
      "\n",
      "7. Capturing Feedback: LangSmith allows developers to gather human feedback on the responses produced by the application and attach feedback scores to logged traces.\n",
      "\n",
      "8. Annotating Traces: LangSmith supports sending runs to annotation queues for annotators to inspect and annotate interesting traces with different criteria.\n",
      "\n",
      "9. Adding Runs to a Dataset: Developers can add runs as examples to datasets to expand test coverage on real-world scenarios and refine the application's performance.\n",
      "\n",
      "10. Production Monitoring: LangSmith enables developers to closely inspect key data points, benchmarking datasets, and monitor application performance with respect to latency, cost, and feedback scores in production.\n",
      "\n",
      "11. Monitoring and A/B Testing: LangSmith provides monitoring charts to track key metrics over time and allows for A/B testing changes in prompt, model, or retrieval strategy.\n",
      "\n",
      "12. Automations: Automations in LangSmith allow developers to perform actions on traces in near real-time, such as scoring traces, sending them to annotation queues, or datasets.\n",
      "\n",
      "Overall, LangSmith offers a comprehensive set of tools and features to support testing and monitoring of LLM applications throughout the development process.\n"
     ]
    }
   ],
   "source": [
    "# Let's clean that up a bit (just print the output)\n",
    "print(out['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6e9b7-3715-4476-a49c-498ceec86472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
