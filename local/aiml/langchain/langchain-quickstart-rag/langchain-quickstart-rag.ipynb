{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05fbdba-c84b-477f-83b5-76246bf49f65",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)\n",
    "\n",
    "We will follow along with the [RAG Quickstart Guide](https://python.langchain.com/docs/use_cases/question_answering/quickstart/) from the LangChain documentation.\n",
    "\n",
    "Building a RAG generally involves the following steps:\n",
    "\n",
    "## Indexing\n",
    "\n",
    "1. Load: Load the data\n",
    "2. Split: Split the data into manageable chunks (helps it fit in the model's context window, ensures we don't include too much irrelevant context)\n",
    "3. Store: Save the data in a searchable format (usually, we will generate embeddings and store them in a vector store)\n",
    "\n",
    "## Retrieval and generation\n",
    "\n",
    "1. Retrieve: given a user input, pull relevant data from our datastore\n",
    "2. Generate: pass the relevant data to the model along with the user's question, generate a response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c68b9d-c9b9-425e-9c0d-01afc767130a",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec84b97-1c41-439f-9e92-4288223fab38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Set up dependencies\n",
    "%pip install --upgrade --quiet pip\n",
    "%pip install --upgrade --quiet langchain langchain-community langchainhub langchain-openai langchain-chroma langchain-openai bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc467c01-47be-4250-802e-85552f1efa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded for simplicity\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2743bc0a-5c53-4a33-8599-12663a70b27e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cec730-26eb-4b24-be60-b5b4c8389524",
   "metadata": {},
   "source": [
    "## 1. Speed Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c664c41-5f41-47ef-a5e2-c9312fdd8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956d0465-a899-47e5-a25b-2db02b450959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38665159-28cf-4be8-a2f1-b68ffcd5876a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique that involves breaking down complex tasks into smaller, more manageable steps. This allows agents or models to tackle difficult tasks by focusing on individual components. Task decomposition can be achieved through prompting techniques like Chain of Thought and Tree of Thoughts, as well as task-specific instructions or human inputs.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1c89c2-a29b-4712-be0b-e31f0b5ad6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cleanup\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af92b6c-5e2e-4dbe-b8ae-4de17f44dc43",
   "metadata": {},
   "source": [
    "## 2. Detailed Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a44eb727-711b-4f3c-8407-673710259c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load blog content from Lilian Wang's GitHub site\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ee9e5d-d6f0-43b5-80b7-0322040f89cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43131"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31773a1d-e083-433a-8c09-e0d4ec755152",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869dccbc-4ddd-46e6-9a6b-c9090b6cb9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - chunk-size breaks text into smaller chunks which will fit in a model's context window\n",
    "# - chunk_overlap limits the chances of accidentally cutting off a statement from related context\n",
    "# - add_start_index makes sure the starting point of a chunk (in the original document) is preserved as a metadata attribute\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa20da89-1fba-49d1-baa9-dd55a577c9f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57e193ca-abb7-4546-af0b-ff29748260ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "424848db-bf26-451f-8667-89dd5aae6589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7056}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d27c25a2-ad63-43a2-996c-9af65ae2ad1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for each chunk and store in a vector store (Chroma)\n",
    "# This way, we can search for chunks which are most similar to the user's input, and use those\n",
    "# as context when responding to the user\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc07b1-b658-4e9e-bb0e-2acdb5c4503f",
   "metadata": {},
   "source": [
    "## Rerieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "478a1b1c-eaff-4d0d-af72-274530877eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the LangChain *Retriever* class to find relevant documents in our vectorstore\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23995e51-acf1-478a-a778-e5d9e5e15250",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Number of docs retrieved: 6 ==\n",
      "== Content of first doc ==\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve some docs, count them, and display an example (page content from doc 0)\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(f'== Number of docs retrieved: {len(retrieved_docs)} ==')\n",
    "print('== Content of first doc ==')\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2395a457-9c66-45a8-9446-9f84fa8a654e",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b46588c-8e51-4820-a9f9-12cd1d5924e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pull a RAG prompt template from the LangChain hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b91edc-3fba-41c6-8d03-0eb7f0777fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate example messages (generic)\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baafc5e0-de0a-40aa-95d9-958c9922c22a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Examine the example message's content\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6abde84-afcd-4721-9074-160e16211dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the RAG chain (gives us streaming, async, and batched calling out of the box, according to the LangChain docs)\n",
    "# Note: code is a repeat of code earlier in the notebook (from the speedrun)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fee3d24e-0bb9-4443-9be2-814027247e46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a process of breaking down complex tasks into smaller, more manageable steps. This technique allows models to think step by step and utilize computation more efficiently. It can be done through simple prompting, task-specific instructions, or with human inputs."
     ]
    }
   ],
   "source": [
    "# Ok, now call the chain\n",
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cebb9633-255e-4e82-bd5b-f6e5c8725787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique where complex tasks are broken down into smaller and simpler steps to be completed more easily. This process involves transforming big tasks into manageable tasks through step-by-step thinking or exploration of multiple reasoning possibilities. Task decomposition can be done through simple prompting, task-specific instructions, or human inputs. Thanks for asking!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We do not have to use the prompt from the LangChain Hub, we can customize\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522ac61-bcc6-4ed6-b492-72171e0b3690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
